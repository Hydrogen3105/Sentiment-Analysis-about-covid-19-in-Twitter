# -*- coding: utf-8 -*-
"""sentiment-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JRKJ5DfAtiizOsPQZZ_DTZvdYh6xq1Vw

## Load Data
"""

import pandas as pd
import numpy as np
import regex as re
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report

train_df = pd.read_csv('./Corona_NLP_train.csv', encoding= 'latin1')
test_df = pd.read_csv('./Corona_NLP_test.csv', encoding= 'latin1')
print('Train shape ', train_df.shape)
print('Test shape ', test_df.shape)

"""## Explore Data"""

train_df.head()

test_df.head()

train_df.info()

test_df.info()

df = pd.concat([train_df, test_df], axis=0)
df.info()

df[190:200]

df['Location'] = df['Location'].fillna('Unknown')
df.info()

df['length_tw'] = df['OriginalTweet'].str.len()

"""### Plotting distributions"""

df['length_tw'].plot(kind='box', vert=False, figsize=(8, 1))

df['length_tw'].plot(kind='hist', figsize=(6, 4), edgecolor='k')

"""## Preprocessing Data"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
train_df['Sentiment'] = le.fit_transform(train_df['Sentiment'])
test_df['Sentiment'] = le.transform(test_df['Sentiment'])

train_df.head()

le.classes_

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

# clean data
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
HTML_TAG_RE = re.compile('<.*?>') 
STOPWORDS = set(stopwords.words('english'))
EMOJI_RE = re.compile("["u"\U0001F600-\U0001F64F"  # emoticons
                         u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                         u"\U0001F680-\U0001F6FF"  # transport & map symbols
                         u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                         u"\U00002702-\U000027B0"
                         u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
URL_RE = re.compile("https?://\S+|www\.\S+")

def clean_text(text):
    text = text.lower() # lowercase text
    text = URL_RE.sub('', text)
    text = HTML_TAG_RE.sub('', text)
    text = EMOJI_RE.sub('', text)
    text = REPLACE_BY_SPACE_RE.sub(' ', text)
    text = BAD_SYMBOLS_RE.sub('', text) 
#    text = re.sub(r'\W+', '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    return text

train_df['OriginalTweet'] = train_df['OriginalTweet'].apply(clean_text)
test_df['OriginalTweet'] = test_df['OriginalTweet'].apply(clean_text)

train_df.head()

from tensorflow.keras.utils import to_categorical
train_labels = to_categorical(train_df['Sentiment'])
test_labels = to_categorical(test_df['Sentiment'])

"""## Modeling"""

from tensorflow.keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense, GlobalMaxPooling1D, Dropout, Bidirectional, GRU
from keras.callbacks import EarlyStopping
from keras.models import load_model

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

MAX_NB_WORDS = 50000
MAX_SEQUENCE_LENGTH = 250
EMBEDDING_DIM = 128

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, 
                      filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', 
                      lower=True)
tokenizer.fit_on_texts(train_df['OriginalTweet'].values)
word_index = tokenizer.word_index
TOTAL_WORDS = len(word_index) + 1

X.shape

train_labels.shape

X = tokenizer.texts_to_sequences(train_df['OriginalTweet'].values)
X = pad_sequences(X, maxlen= MAX_SEQUENCE_LENGTH)

X_test = tokenizer.texts_to_sequences(test_df['OriginalTweet'].values)
X_test = pad_sequences(X_test, maxlen= MAX_SEQUENCE_LENGTH)

print("padding sequence: ", X[1])
print("Original before padding: ", train_df['OriginalTweet'][1])

"""### LSTM
#### create first model
"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout= 0.2, recurrent_dropout=0.2))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(5, activation= 'softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(X, train_labels,
          epochs=10, 
          batch_size=64,
          validation_split=0.1
          )
model.save('lstm_model.h5')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12, 4))
plt.figure(figsize=(10,3))

ax1.set_title('Loss')
ax1.plot(history.history['loss'], label='train')
ax1.plot(history.history['val_loss'], label='val')
ax1.legend()

ax2.set_title("Accuracy")
ax2.plot(history.history['accuracy'], label='train')
ax2.plot(history.history['val_accuracy'], label='val')
ax2.legend()

"""#### evaluate first model"""

model = load_model('lstm_model.h5')

model.evaluate(X_test, test_labels)

# Predict test datset
model_pred = np.round(model.predict(X_test))
model_report = classification_report(test_labels, model_pred)

print(model_report)

text = ["I am so grateful to doctors for curing people from COVID-19", "What the hell! ,government staffs didn't do something to help us."]
text = [clean_text(s) for s in text]

lstm_test_1 = tokenizer.texts_to_sequences(text)
lstm_test_1 = pad_sequences(lstm_test_1, maxlen= MAX_SEQUENCE_LENGTH)

lstm_test_pred_1 = np.round(model.predict(lstm_test_1))
for sentence, pred in zip(text, lstm_test_pred_1):
  print(f"{sentence} has sentimental level at {le.classes_[np.argmax(pred)]}")

"""#### create second model"""

model_2 = Sequential()
model_2.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model_2.add(LSTM(100, dropout=0.2, recurrent_dropout= 0.2, return_sequences=True))
model_2.add(GlobalMaxPooling1D())
model_2.add(Dense(64, activation= 'relu'))
model_2.add(Dropout(0.2))
model_2.add(Dense(32, activation= 'relu'))
model_2.add(Dense(5, activation='softmax'))

model_2.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])
model_2.summary()

history_2 = model_2.fit(X, train_labels,
          epochs=10, 
          batch_size=64,
          validation_split=0.1,
          callbacks = [EarlyStopping(monitor= 'val_loss',
                                      patience= 3,
                                     min_delta= 0.0001)]
          )

model_2.save('lstm_model_2.h5')

"""#### evaluate second model"""

#### Evalute GRU Model
model_2 = load_model('lstm_model_2.h5')

model_2.evaluate(X_test, test_labels)

# Predict test datset
model_pred_2 = np.round(model_2.predict(X_test))
model_report_2 = classification_report(test_labels, model_pred_2)
print(model_report_2)

text = ["I am so grateful to doctors for curing people from COVID-19", "What the hell! ,government staffs didn't do something to help us."]
text = [clean_text(s) for s in text]

lstm_test_2 = tokenizer.texts_to_sequences(text)
lstm_test_2 = pad_sequences(lstm_test_2, maxlen= MAX_SEQUENCE_LENGTH)

lstm_test_pred_2 = np.round(model_2.predict(lstm_test_2))
for sentence, pred in zip(text, lstm_test_pred_2):
  print(f"{sentence} has sentimental level at {le.classes_[np.argmax(pred)]}")

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12, 4))
plt.figure(figsize=(10,3))

ax1.set_title('Loss')
ax1.plot(history_2.history['loss'], label='train')
ax1.plot(history_2.history['val_loss'], label='val')
ax1.legend()

ax2.set_title("Accuracy")
ax2.plot(history_2.history['accuracy'], label='train')
ax2.plot(history_2.history['val_accuracy'], label='val')
ax2.legend()

"""#### test with new text"""

model = load_model('lstm_model.h5')
model_2 = load_model('lstm_model_2.h5')

text = ["I am so grateful to doctors for curing people from COVID-19", "What the hell! ,government staffs didn't do something to help us."]
text = [clean_text(s) for s in text]
text_sequence = tokenizer.texts_to_sequences(text)
text_sequence = pad_sequences(text_sequence, maxlen=MAX_SEQUENCE_LENGTH)
print(text_sequence[0], text[0])

new_data_pred = model.predict(text_sequence)
class_pred = [le.classes_[np.argmax(pred)] for pred in new_data_pred]
for sentence, class_pred in zip(text, class_pred):
  print(f"{sentence} has sentimental level at {class_pred}")

new_data_pred_2 = model_2.predict(text_sequence)
class_pred = [le.classes_[np.argmax(pred)] for pred in new_data_pred]
for sentence, class_pred in zip(text, class_pred):
  print(f"{sentence} has sentimental level at {class_pred}")

"""### Bidirectional LSTM"""

BLSTM_model = Sequential()
BLSTM_model.add(Embedding(TOTAL_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
BLSTM_model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.1)))
BLSTM_model.add(GlobalMaxPooling1D())
BLSTM_model.add(Dropout(0.5))
BLSTM_model.add(Dense(5, activation='softmax'))

optimizer=Adam(learning_rate=0.00025)
BLSTM_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
BLSTM_model.summary()

history_BLSTM = BLSTM_model.fit(X, train_labels,
                    epochs=5, 
                    batch_size=8,
                    validation_split=0.1,
                    ) 
BLSTM_model.save('Blstm_model.h5')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12, 4))
plt.figure(figsize=(10,3))

ax1.set_title('Loss')
ax1.plot(history_BLSTM.history['loss'], label='train')
ax1.plot(history_BLSTM.history['val_loss'], label='val')
ax1.legend()

ax2.set_title("Accuracy")
ax2.plot(history_BLSTM.history['accuracy'], label='train')
ax2.plot(history_BLSTM.history['val_accuracy'], label='val')
ax2.legend()

"""#### Evalute Bidirectional LSTM Model"""

BLSTM_model = load_model('Blstm_model.h5')

BLSTM_model.evaluate(X_test, test_labels)

# Predict test datset
model_pred = np.round(BLSTM_model.predict(X_test))
model_report = classification_report(test_labels, model_pred)

print(model_report)

"""#### Test with new text"""

model = load_model('lstm_model.h5')

text = ["I am so grateful to doctors for curing people from COVID-19", "What the hell! ,government staffs didn't do something to help us."]
text = [clean_text(s) for s in text]
text_sequence = tokenizer.texts_to_sequences(text)
text_sequence = pad_sequences(text_sequence, maxlen=MAX_SEQUENCE_LENGTH)
print(text_sequence[0], text[0])

new_data_pred = model.predict(text_sequence)
class_pred = [le.classes_[np.argmax(pred)] for pred in new_data_pred]
for sentence, class_pred in zip(text, class_pred):
  print(f"{sentence} has sentimental level at {class_pred}")

new_data_pred_2 = model_2.predict(text_sequence)
class_pred = [le.classes_[np.argmax(pred)] for pred in new_data_pred]
for sentence, class_pred in zip(text, class_pred):
  print(f"{sentence} has sentimental level at {class_pred}")

"""### GRU"""

GRU_model = Sequential()
GRU_model.add(Embedding(TOTAL_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
GRU_model.add(GRU(64))
GRU_model.add(Dropout(0.5))
GRU_model.add(Dense(32, activation='relu', kernel_regularizer='l2'))
GRU_model.add(Dense(5, activation='softmax'))

optimizer=Adam(learning_rate=0.00025)
GRU_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
GRU_model.summary()

history_GRU = GRU_model.fit(X, train_labels,
                    epochs=3, 
                    batch_size=8,
                    validation_split=0.1,
                    ) 
GRU_model.save('GRU_model.h5')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12, 4))
plt.figure(figsize=(10,3))

ax1.set_title('Loss')
ax1.plot(history_GRU.history['loss'], label='train')
ax1.plot(history_GRU.history['val_loss'], label='val')
ax1.legend()

ax2.set_title("Accuracy")
ax2.plot(history_GRU.history['accuracy'], label='train')
ax2.plot(history_GRU.history['val_accuracy'], label='val')
ax2.legend()

"""#### Evalute GRU Model"""

GRU_model = load_model('GRU_model.h5')

GRU_model.evaluate(X_test, test_labels)

# Predict test datset
model_pred = np.round(GRU_model.predict(X_test))
model_report = classification_report(test_labels, model_pred)

print(model_report)

"""### Logistic Regression"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.linear_model import LogisticRegressiona

cv = CountVectorizer()
train_vec = cv.fit_transform(train_df['OriginalTweet'])

logistic_model = LogisticRegression()
logistic_model.fit(train_vec, train_df['Sentiment'].values)

tfidf = TfidfTransformer()
tfidf_train_vec = tfidf.fit_transform(train_vec)

logistic_model_tfidf = LogisticRegression()
logistic_model_tfidf.fit(tfidf_train_vec,
                         train_df['Sentiment'].values)

"""#### test logistic regression model"""

test_vec = cv.transform(test_df['OriginalTweet'])
tfidf_test_vec = tfidf.fit_transform(test_vec)

test_predictions = logistic_model.predict(test_vec)
print(classification_report(test_predictions, test_df['Sentiment'].values))

test_predictions = logistic_model_tfidf.predict(test_vec)
print(classification_report(test_predictions, test_df['Sentiment'].values))

text = ["I am so grateful to doctors for curing people from COVID-19", "What the hell! ,government staffs didn't do something to help us."]
text = [clean_text(s) for s in text]
text_vec = cv.transform(text)
text_pred = logistic_model.predict(text_vec)

for sentence, pred in zip(text, text_pred):
  print(f"{sentence} has sentimental level at {le.classes_[pred]}")

text_tfidf_pred = logistic_model_tfidf.predict(text_vec)
for sentence, pred in zip(text, text_tfidf_pred):
  print(f"{sentence} has sentimental level at {le.classes_[pred]}")

"""### KMeans"""

from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_df=0.5, max_features=2000,
                                 min_df=2, stop_words='english',
                                 use_idf=True)
X = vectorizer.fit_transform(train_df['OriginalTweet'])

km = KMeans(n_clusters=5, max_iter=100, n_init=55)
km.fit(X)

order_centroids = km.cluster_centers_.argsort()[:, ::-1]

terms = vectorizer.get_feature_names()
for i in range(5):
    print("Cluster %d:" % i, end='')
    for ind in order_centroids[i, :30]:
        print(' %s' % terms[ind], end='')
    print()

"""### NaiveBayes

TF-IDF
"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words=None)
X_train_tfidf = tfidf.fit_transform(train_df['OriginalTweet'] )
X_test_tfidf = tfidf.transform(test_df["OriginalTweet"])

from sklearn.naive_bayes import MultinomialNB
naive_bayes_tfidf = MultinomialNB()
naive_bayes_tfidf.fit(X_train_tfidf, train_df['Sentiment'].values)



"""CountVectorizer"""

cv = CountVectorizer()
X_train_cv = cv.fit_transform(train_df['OriginalTweet'] )
X_test_cv = cv.transform(test_df["OriginalTweet"])

naive_bayes_cv = MultinomialNB()
naive_bayes_cv.fit(X_train_cv, train_df['Sentiment'].values)

nb_cv_predict = naive_bayes_cv.predict(X_test_cv)

"""### Test Naive Bayes"""

nb_tfidf_predict = naive_bayes_tfidf.predict(X_test_tfidf)
print(classification_report(nb_tfidf_predict, test_df['Sentiment'].values))

nb_cv_predict = naive_bayes_cv.predict(X_test_cv)
print(classification_report(nb_cv_predict, test_df['Sentiment'].values))

text = ["I am so grateful to doctors for curing people from COVID-19", "What the hell! ,government staffs didn't do something to help us."]
text = [clean_text(s) for s in text]
text_vec = tfidf.transform(text)
text_tfidf_pred = naive_bayes_tfidf.predict(text_vec)

for sentence, pred in zip(text, text_tfidf_pred):
  print(f"{sentence} has sentimental level at {le.classes_[pred]}")

text_cv_pred = naive_bayes_cv.predict(text_vec)
for sentence, pred in zip(text, text_cv_pred):
  print(f"{sentence} has sentimental level at {le.classes_[pred]}")

